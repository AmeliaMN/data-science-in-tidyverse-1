---
title: "Stats in R"
author: Amelia McNamara
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, error = TRUE)
```

### Install some extra packages (if needed)

```{r, eval=FALSE}
install.packages("modelr")
install.packages ("janitor")
install.packages("skimr")
```

### Load packages

```{r}
library(tidyverse)
library(modelr)
library(broom)
library(janitor)
library(skimr)
```

### Load data

We'll be using a table of simplified school-level data regarding the racial makeup of schools and their scores on the PARCC standardized test.

(It's outside our purview today, but of course normally we would use best practice and study the data dictionary/documentation before we do anything else).

```{r}
schools_data <- read_csv("ILschools.csv")
```

The readr package decisions seem reasonable. Let's have a look at the top of our table. Scroll through to the right, paying special attention to our percentage columns. 


### Fix column headers

The janitor package fixes a lot of common data hygeine problems. In this case, we'll change these hideous and inconsistent column names to conform with R conventions

```{r}
names(schools_data)
schools_data <- schools_data %>% 
  clean_names()
names(schools_data)
```


### Learn a bit about our columns from the get-go


```{r}
skim(schools_data)
```

We can see that the various columns have very different means. We can also see that the means and medians can be very different. This indicates to us that the distributions are different, and not normal.

## Tidy data for modeling

We've talked about some aspects of "tidy" data, but this dataset is untidy in a new way-- it has observations in it that are more than one type of "thing." 

```{r}
schools_data %>%
  count(type)
```

In this case, some rows are high schools, others are middle schools, others are elementary schools... and others are charter schools. We shouldn't do data analysis on this entire dataset, because it would be comparing apples to oranges. 

So, let's focus just on elementary schools (the biggest category!). Practice filtering for just elementary schools.

```{r}
elementary_schools <- 
```


## Exploring distributions with ggplot

As we've seen, `ggplot2` is a very powerful graphics library! It can be used to make production-quality graphics, but it's also a good tool in your toolbox for doing quick checks on your data.  

First, let's generate some (fake!) data to look at what's known as a normal distribution.

```{r}
set.seed(1) # makes the randomness across computers
df <- tibble(PF = 10*rnorm(10000))
ggplot(df, aes(x = PF)) + 
    geom_histogram(aes(y =..density..),
                   breaks = seq(-50, 50, by = 5), 
                   colour = "black", 
                   fill = "deepskyblue") +
stat_function(fun = dnorm, args = list(mean = mean(df$PF), sd = sd(df$PF)))
```




```{r}
df %>%
  summarize(mean = mean(PF), median = median(PF))
```

In this distribution, the mean (aka 'average') and the median are essentially equal (and, almost equal to zero), and both sides are symmetrical around them. But this is unusual in the real world. 



Let's start with the distribution of test scores, which we'll make with ggplot. 

```{r}
ggplot(elementary_schools, aes(x = parc_cpct)) +
  geom_histogram(binwidth = 10)
```

What does this tell us about the distribution of school test-passing rates? This is called skew.

Next let's look at the distribution of the percent black variable

```{r}
ggplot(elementary_schools, aes(pct_black)) +
  geom_histogram(binwidth = 10)
```

What does this shape tell us about schools in Illinois?

What are some other areas of life where we might find this distribution?

You try: make a histogram of the distribution of pct_low_inc

```{r}

```

## Looking at relationships between two variables

Now let's plot the relationship between the percent of a school's students who are low-income and the percent who achieved proficiency on the PARCC exam.

```{r}
ggplot(elementary_schools, aes(x = pct_low_inc, y = parc_cpct)) +
  geom_point(color = "turquoise", alpha = .6)
```

We can see that in general a higher rate of low-income students at a school is associated with a lower rate of passing the exam.

Let's formalize this by adding a line to our plot. `ggplot2` can fit a "line of best fit" by using a particular `geom_` function, `geom_smooth`

```{r}
ggplot(elementary_schools, aes(x = pct_low_inc, y = parc_cpct)) +
  geom_point(color = "turquoise", alpha = .6) + 
  geom_smooth(method = "lm")
```

This confirms what we could see visually. But, if you were a statistician you would want the equation of that line, so you could interpret the slope and y-intercept. Unfortunately, we can't get that information from the plot, so we need to "fit" the model separately. 

We'll use the `lm()` command. Much like other commands in R, you can run `lm()` by itself without saving it into a variable, 

```{r}
lm(parc_cpct ~ pct_low_inc, data = elementary_schools)
```

The syntax here is $y~x$, or response_variable~explanatory variable. A response variable is one that you think might "respond" in some way. An explanatory variable is one you think might "explain" the variation in the response. In this case, we believe the percentage of low income students might help explain the variability in the percentage of students who pass the test. 

Remember y = mx + b ? What does the output above tell us? Let's try interpreting those model coefficients. Here are the generic sentences I prefer:

**Intercept** If the value of [explanatory variable] was zero, our model would predict [response variable] to be [intercept value].

**Slope** For a one-[unit] increase in [explanatory variable], our model would predict a [slope value]-[unit] [increase/decrease] in [response variable]. 

Let's apply that to this model:







As with many things in R, though, you will probably want to save your model object with a name, so you can use it later. 

```{r}
low_inc_mod <- lm(parc_cpct ~ pct_low_inc, data = elementary_schools)
```

When we run this code, nothing prints out. But, a new object should appear in our RStudio Environment pane. Now, we can run R functions on that model object. The most useful one is `summary()`,

```{r}
summary(low_inc_mod)
```
The same model coefficients are shown here, but there is also a lot more information about things like the $R^2$ value. 


You try: Run a linear regression that shows the relationship between the percent white and the percent who passed the exam. What do we learn from this? How do we put it into words?

```{r}

```




### Multiple regression

Does a school being in Chicago affect these results? First, let's consider this visually. 

```{r}
ggplot(elementary_schools, aes(pct_low_inc, parc_cpct, color = chi)) +
  geom_point(alpha = .5)
```

It seems like there might be something going on, but it's a little hard to tell. Let's try adding a line,


```{r}
ggplot(elementary_schools, aes(pct_low_inc, parc_cpct, color = chi)) +
  geom_point(alpha = .5) +
  geom_smooth(method = "lm")
```

It does look like there is some difference there. By default, when you add a `geom_smooth` to a plot that has a color variable, it makes two lines. And, it lets them have both different slopes and different intercepts. Typically, though, when we are modeling we want to force the slopes to be the same and only vary the intercepts, unless we have a very good intuition about why the slopes would be different. 

Let's run a new regression that includes the 'chi' variable, and then summarize output. If we want to use multiple variables in our model, we add them with the `+` sign. 

```{r}
multi_mod <- lm(parc_cpct ~ pct_low_inc + chi, data = elementary_schools)
summary(multi_mod)
```

How do we interpret these coefficients? The first problem is I've only given you sample sentences for the intercept and for one quantitative variable. But now we have a categorical variable, as well!

The generic sentence for a categorical variable is

Compared to [the reference group], our model would predict that [the group we're considering] to have a [response variable] that is [coefficient value]-[units] [higher/lower]. 

Let's apply that here. 



There's one more wrinkle, though. We have more than one variable, and when R fits the model it is finding the best fit lines considering both variables at once. So, we need to complicate our interpretation sentences to reflect that. Statisticians disagree, but I think adding "holding all else constant" to the end of the sentence is a fine way to go. Let's try that here:



### Cleaning up model output

The output from `summary()` is pretty ugly. We can make it more useful using the broom packaage.

```{r}
tidy(multi_mod)
```

Now it's a data frame, instead of that weird model object. We can work with this using the data-wrangling tools we learned from `dplyr`. 

```{r}
tidy(multi_mod) %>%
  select(term, estimate)
```


You practice. Run a linear model where our predictors are pct_white and chi. Then clean up your output using broom.

```{r}

```

### Fitted values

If we feed a model through the augment() function, it will make a dataset that contains our response variable, the explanatory variable(s), fitted (or "predicted") values, and much more, all in a tidy format. 

```{r}
augment(multi_mod)
```


A fitted, or predicted, value is the percent PARCC proficiency our model suggests would be expected for this school given the terms in our model (in this case the percent of a school's students who are low-income and whether the school is in Chicago).

A residual is the difference between the **actual** PARCC proficiency of a school, and the **fitted** value our model predicted. We write this as $observed - expected$ or $y_i-\hat{y}_i$. We can see residuals by scrolling over in the augmented dataframe. 

The difference between the fitted value and the actual value can be interpreted as whether a school is over- or under-performing. We can plot this.

To keep our plot from being too busy, let's filter out values that are not outliers.

```{r}
multi_mod_augment <- augment(multi_mod) 
multi_mod_outliers <- multi_mod_augment %>% 
  filter(.resid < -20 | .resid > 20)
```



```{r}
ggplot(multi_mod_outliers, aes(x = pct_low_inc, y = parc_cpct)) +
  geom_segment(aes(xend = pct_low_inc, yend = .fitted), alpha = .2) +
  geom_point(aes(color = .resid),alpha = .8) +
  scale_color_gradient2(low = "blue", mid = "white", high = "red") +
  guides(color = FALSE) +
  geom_point(aes(y = .fitted)) +
  facet_grid(~ chi)
```

# Looks like we've got some schools with unexpected test scores!

Let's find them.

```{r}
multi_mod_outliers %>% 
  arrange(.resid)
```

From this, we can see that row 106 is particularly far off from our predictions. But, we don't know which school it is, because we lost the variables that weren't used in making our model. Luckily, there's a way to fix that! We can pass the dataset as an argument to the `augment` function.

```{r}
multi_mod_outliers <- augment(multi_mod, elementary_schools) %>% 
  filter(.resid < -20 | .resid > 20)
multi_mod_outliers %>%
  arrange(.resid)
```

Story language:

Swan Hillman Elementary School underscored what would be expected for a school with a similar number of low-income students in Chicago by 47 percentage points. 

We can always `arrange()` the other direction, too. 

```{r}
multi_mod_outliers %>%
  arrange(desc(.resid))
```


Eisenhower Elementary outscored what would be expected for a similar school with mostly low-income students by 73 percentage points.

### Interpreting model fit

Looking at the plots, it seems like we have some big residuals. Those might be the story in themselves! But, if we are trying to find a good predictive model we are interested in the percentage of variability in the response explained by the explanatory variable(s). Let's look back at summaries of our models.

```{r}
summary(low_inc_mod)
```
The "multiple R-squared" here is 0.6177. So, our model using just `pct_low_inc` can explain 62% of the variability in PARC scores. That's pretty good! If it was only 10%, we might worry. (The amount of variability you consider to be "good" is highly subjective, and depends on the context of the data. If we were predicting earthquakes, 10% of the variability would be amazing! In some scientific fields, anything less than 90% is bad. In social science contexts 60% is usually considered to be good.)

```{r}
summary(multi_mod)
```
This model is even better! It explains 64% of the variability in PARC scores. 

One caveat is that multiple R-squared values always increase as you add terms to your model. So, if you want to compare two models you should use adjusted R-squared, which doesn't have as nice of an interpretation sentence, but controls for the effect of adding new variables. Looking at our adjusted $R^2$, this model is still better. 


### Assessing model conditions

Linear models are only reliable for predictions and inference when their conditions are met. The conditions can be remembered by the acronym LINE.

- Linearity
- Independence
- Normality
- Equality of variance

I always say independence is the "thinking condition," because you can't assess it using plots. Instead, you have to think about whether you believe observations in your data are independent from one another. Do we think a particular elementary school is independent from another elementary school? Perhaps not, if they are in the same district or share a principal. But, there is little to be done to correct this problem once we have data collected, and in cases like this, no way to collect the data differently.

For the other three conditions, we need to look at plots of residuals. The first (and most important!) residual plot shows the relationship between residuals and fitted values. 

```{r}
  ggplot(multi_mod_augment, aes(x=.fitted, y=.resid)) +
  geom_point()+
  geom_smooth(se=FALSE)
```

In this plot, we are looking for linearity (we want that middle line to be nice and flat) and equality of variance (we want the band of residuals to be the same width all the way across.)

In this case, linearity looks okay to me, but equality of variance seems to be violated. This means we actually shouldn't make predictions from this model! 

The other condition we can assess with a plot is normality. We want our residuals to be normally distributed. There are a couple ways to check this.

One is to make a histogram, just like we did at the beginning of this document. 

```{r}
ggplot(multi_mod_augment, aes(x = .resid)) + 
    geom_histogram(aes(y =..density..),
                  # breaks = seq(-50, 50, by = 5), 
                   colour = "black", 
                   fill = "deepskyblue") +
stat_function(fun = dnorm, args = list(mean = mean(multi_mod_augment$.resid), sd = sd(multi_mod_augment$.resid)))
```
To me, this looks to be non-normal. I can see a skew there. The other option (statisticians love this one!) is to look at a "QQ" or "quantile-quantile" plot to assess normality. 

```{r}
ggplot(multi_mod_augment, aes(sample=.resid)) +
  stat_qq() +
  stat_qq_line()
```

For the normality condition to be upheld, we want the dots to fall along the line. In this case, the dots are pulling away from the line in a "s-shape." A "c-shape" would also be bad. 

In order to have a model we can trust, we need to fix these violations. There are rules of thumb to help (searching for "Tukey's bulging rule" to learn more), but in many cases what you need is a log transformation. Sadly, in this case I don't think that's going to fix it. 

One way to tell is to make a histogram of a variable in your model that is non-normal, and then try applying the transformation to see if the resulting histogram looks better. Most of the time, it does, but not in this case! 

```{r}
ggplot(elementary_schools) + geom_histogram(aes(x=parc_cpct))
ggplot(elementary_schools) + geom_histogram(aes(x=log(parc_cpct)))
```

### Logistic regression

Another common type of model is a logistic regression model. For this type of model, our response variable (the thing we think might "respond") needs to be binary: a yes/no, TRUE/FALSE, or 0/1 variable. We can still use any type of explanatory variable as a predictor in the model, but the interpretation of the model will be different. In our dataset, we don't have a variable that is binary, but we can create one (this can be a bad practice, so we're mostly doing it for illustration).

For another `dplyr` review, add a new column to the `elementary_schools` data called `good_school`, which is whether or not more than half of the students have passed the test.

```{r}

```

In order to run logistic regression, we need to use a different function-- this one is called `glm()`, for Generalized Linear Models. `glm` by default produces a linear regression model, so we can try the same simple regression model we started with,

```{r}
glm(parc_cpct ~ pct_low_inc, data = elementary_schools)
```

What is different about the output?



In order to run a logistic regression model, we need to use a binary response variable (`good_school`, in this case), and change the "family" of the model to be "binomial" (this is the theoretical probability model that goes with logistic regression, I'm happy to explain more offline).

Now let's use two variables to predict whether a school is "good".

```{r}
multi_logistic <- glm(good_school~pct_low_inc + chi, data=elementary_schools, family="binomial")
```

We can use our summary function again to see the output of our regression.

```{r}
summary(multi_logistic)
```


We can't interpret our coefficients the same way anymore. In fact, these numbers represent a "space" that is hard for our brains to reason about-- log-odds space. We would rather think in odds space or probability space. (More about this [here](https://www.amelia.mn/sds291/labs/lab_logistic_regression.html) and [here](https://www.amelia.mn/sds291/lectures/19_logistic.pdf))

Again, we can `augment()` our model to help us think in a better space. 

```{r}
multi_logistic_augmented <- augment(multi_logistic, elementary_schools)
```

If we plot our data, we get something that looks linear-- but it's in log-odds space. 

```{r}
ggplot(multi_logistic_augmented) + geom_point(aes(x=pct_low_inc, y=.fitted, color=chi))
```

We'd rather think about probabilities or odds. Let's `mutate` to add those.

```{r}
multi_logistic_augmented <- multi_logistic_augmented %>%
  mutate(odds = exp(.fitted), probability = odds / (1 + odds))
```

Now, we can plot in those "spaces"

```{r}
ggplot(multi_logistic_augmented) + geom_point(aes(x=pct_low_inc, y=odds, color=chi))
ggplot(multi_logistic_augmented) + geom_point(aes(x=pct_low_inc, y=probability, color=chi))
```


The probability space is the one that most accurately shows how our model looks. For two schools with the same `pct_low_inc` our model assigns a much higher probability of passing the rest for schools in Chicago than schools outside of Chicago. 

Let's look back at our model,

```{r}
summary(multi_logistic)
```

How can we interpret those coefficients? They have been given to us in log-odds space, so if we want, we can interpret them using the same generic sentences I gave you about coefficients in linear regression.

"For a one-unit increase in `pct_low_inc`, we would expect to see a 0.09086-unit decrease in log-odds of being a good school, holding Chicago status constant."

"Compared to schools not in Chicago, we would expect the log-odds of being a good school to be 2.00 units higher, holding `pct_low_inc` constant."

Like I said, this "space" doesnt' make much intuitive sense. But, through some algebra that I'm not going to demonstrate, there is a way to make these coefficients easier to interpret, by taking $e^{coefficient}$. In R, we can raise e to a power by using the `exp()` function.

```{r}
exp(1)
exp(2)
```
More useful is to exponentiate our coefficients,

```{r}
exp(coef(multi_logistic))
```

These numbers are more interpretable. Here are the generic sentences:

"For a one-[unit] increase in [explanatory variable], the odds of [response variable success] are [coefficient value] times [higher/lower], holding [all else] constant."

"Compared to [non-reference group], the odds of [response variable success] are [coefficient value] times [higher/lower], holding [all else] constant." 

Let's apply that to these coefficients:



Does that make sense?


### Assessing a logistic regression model

Deciding if a logistic regression model is good is more difficult, because it no longer has residuals. Either a school is "good" or not, and either we predict it is good or not. One way to consider how good the model is is to see if you did better than the mean.

```{r}
elementary_schools %>%
  count(good_school) %>%
  drop_na()%>%
  mutate(prop = n/sum(n))
```
So, most of our schools did not achieve the 50% passing rate. In fact, 79% of cases are not "good schools." So, we could get 79% of our predictions to be correct just by predicting every single school will have less than a 50% passing rate. 

Did our model do better than that? Let's create a binary variable out of our predicted probability for each school. 

```{r}
multi_logistic_augmented <- multi_logistic_augmented %>%
  mutate(predict_good = if_else(probability>0.5, TRUE, FALSE))
```

```{r}
multi_logistic_augmented %>%
  group_by(good_school, predict_good) %>%
  summarize(n=n())
```
So, we got our predictions right

```{r}
(1644+297)/(1644+122+168+297)
```

87% of the time! That's pretty good, we definitely beat the mean. 

## More models

There are many more models out there! However, the challenge is always explaining them to a broad audience. I have seen linear regression used successfully in news stories pretty regularly, but logistic regression is a harder sell, and more complicated models even more challenging.


<!-- These materials modified from material presented by Olga Pierce, University of Nebraska-Lincoln, at NICAR 2019 -->






